{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1007a6-c7a9-4a49-840e-db3605b63641",
   "metadata": {},
   "source": [
    "## Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e6215-ee09-4afd-b5c9-22735de86b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from iscak_core import ISCAkCore\n",
    "\n",
    "# Configurações\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Configuração de runs\n",
    "N_RUNS = 50\n",
    "BASE_SEED = 42\n",
    "MISSING_RATES = [0.10, 0.20, 0.30, 0.40]\n",
    "MECHANISMS = ['MCAR', 'MAR', 'MNAR']\n",
    "METHOD_ORDER = ['ISCA-k', 'KNN', 'MICE', 'MissForest']\n",
    "\n",
    "# Gerar seeds dinamicamente\n",
    "SEEDS = [BASE_SEED + run for run in range(N_RUNS)]\n",
    "\n",
    "# Cores consistentes para métodos (Plotly)\n",
    "METHOD_COLORS = {\n",
    "    'ISCA-k': '#2E86AB',      # Azul\n",
    "    'KNN': '#A23B72',         # Roxo\n",
    "    'MICE': '#F18F01',        # Laranja\n",
    "    'MissForest': '#C73E1D'   # Vermelho\n",
    "}\n",
    "\n",
    "print(\"Setup inicial completo\")\n",
    "print(f\"N_RUNS: {N_RUNS}\")\n",
    "print(f\"Seeds: {BASE_SEED} a {BASE_SEED + N_RUNS - 1}\")\n",
    "print(f\"Missing rates: {[int(r*100) for r in MISSING_RATES]}%\")\n",
    "print(f\"Mecanismos: {MECHANISMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eb26de-0e71-43d4-9463-88efff4fe28b",
   "metadata": {},
   "source": [
    "## Gerar MCAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cdf170-edb0-4fbf-b176-40145b729e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_mcar(data: pd.DataFrame, missing_rate: float, seed: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Introduz missings MCAR (Missing Completely At Random).\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame original\n",
    "        missing_rate: Proporção de missings (0 a 1)\n",
    "        seed: Seed para reprodutibilidade\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com missings introduzidos\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data_missing = data.copy()\n",
    "    mask = np.random.rand(*data.shape) < missing_rate\n",
    "    data_missing = data_missing.mask(mask)\n",
    "    return data_missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b0412-5dbe-4538-81e2-8791984f5ca8",
   "metadata": {},
   "source": [
    "## Métricas de Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67ca2d-c8c1-4fca-8208-acb9a1e06162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data_original: pd.DataFrame, \n",
    "                     data_imputed: pd.DataFrame, \n",
    "                     missing_mask: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calcula métricas de avaliação para imputação.\n",
    "    Retorna tanto métricas agregadas quanto métricas por coluna.\n",
    "    \n",
    "    Args:\n",
    "        data_original: DataFrame original (ground truth)\n",
    "        data_imputed: DataFrame após imputação\n",
    "        missing_mask: Máscara booleana dos valores que eram missing\n",
    "    \n",
    "    Returns:\n",
    "        Dict com:\n",
    "        - R2, Pearson, NRMSE (numéricas) e Accuracy (categóricas) - AGREGADOS\n",
    "        - column_metrics: dict com métricas individuais por coluna\n",
    "    \"\"\"\n",
    "    numeric_cols = data_original.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = data_original.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    metrics = {\n",
    "        'R2': np.nan,\n",
    "        'Pearson': np.nan,\n",
    "        'NRMSE': np.nan,\n",
    "        'Accuracy': np.nan,\n",
    "        'column_metrics': {}\n",
    "    }\n",
    "    \n",
    "    # ===== MÉTRICAS NUMÉRICAS =====\n",
    "    r2_scores = []\n",
    "    pearson_scores = []\n",
    "    nrmse_scores = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        col_mask = missing_mask[col]\n",
    "        \n",
    "        if not col_mask.any():\n",
    "            continue\n",
    "        \n",
    "        orig_vals = data_original.loc[col_mask, col].values\n",
    "        imp_vals = data_imputed.loc[col_mask, col].values\n",
    "        \n",
    "        # Remover possíveis NaN remanescentes\n",
    "        valid = ~(pd.isna(orig_vals) | pd.isna(imp_vals))\n",
    "        orig_vals = orig_vals[valid]\n",
    "        imp_vals = imp_vals[valid]\n",
    "        \n",
    "        if len(orig_vals) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Inicializar métricas desta coluna\n",
    "        col_metrics = {\n",
    "            'type': 'numeric',\n",
    "            'R2': np.nan,\n",
    "            'NRMSE': np.nan,\n",
    "            'Accuracy': np.nan\n",
    "        }\n",
    "        \n",
    "        # R²\n",
    "        if np.var(orig_vals) > 1e-10:\n",
    "            col_r2 = r2_score(orig_vals, imp_vals)\n",
    "            r2_scores.append(col_r2)\n",
    "            col_metrics['R2'] = col_r2\n",
    "        \n",
    "        # Pearson\n",
    "        if len(orig_vals) >= 3 and np.var(orig_vals) > 1e-10 and np.var(imp_vals) > 1e-10:\n",
    "            pearson_scores.append(pearsonr(orig_vals, imp_vals)[0])\n",
    "        \n",
    "        # NRMSE\n",
    "        rmse = np.sqrt(mean_squared_error(orig_vals, imp_vals))\n",
    "        value_range = orig_vals.max() - orig_vals.min()\n",
    "        if value_range > 1e-10:\n",
    "            col_nrmse = rmse / value_range\n",
    "            nrmse_scores.append(col_nrmse)\n",
    "            col_metrics['NRMSE'] = col_nrmse\n",
    "        \n",
    "        metrics['column_metrics'][col] = col_metrics\n",
    "    \n",
    "    if r2_scores:\n",
    "        metrics['R2'] = np.mean(r2_scores)\n",
    "    if pearson_scores:\n",
    "        metrics['Pearson'] = np.mean(pearson_scores)\n",
    "    if nrmse_scores:\n",
    "        metrics['NRMSE'] = np.mean(nrmse_scores)\n",
    "    \n",
    "    # ===== MÉTRICAS CATEGÓRICAS =====\n",
    "    accuracy_scores = []\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        col_mask = missing_mask[col]\n",
    "        \n",
    "        if not col_mask.any():\n",
    "            continue\n",
    "        \n",
    "        orig_vals = data_original.loc[col_mask, col]\n",
    "        imp_vals = data_imputed.loc[col_mask, col]\n",
    "        \n",
    "        # Remover possíveis NaN remanescentes\n",
    "        valid = ~(orig_vals.isna() | imp_vals.isna())\n",
    "        if valid.sum() < 1:\n",
    "            continue\n",
    "        \n",
    "        orig_vals = orig_vals[valid]\n",
    "        imp_vals = imp_vals[valid]\n",
    "        \n",
    "        col_accuracy = (orig_vals == imp_vals).mean()\n",
    "        accuracy_scores.append(col_accuracy)\n",
    "        \n",
    "        # Guardar métricas desta coluna\n",
    "        metrics['column_metrics'][col] = {\n",
    "            'type': 'categorical',\n",
    "            'R2': np.nan,\n",
    "            'NRMSE': np.nan,\n",
    "            'Accuracy': col_accuracy\n",
    "        }\n",
    "    \n",
    "    if accuracy_scores:\n",
    "        metrics['Accuracy'] = np.mean(accuracy_scores)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b72bc4-3ea6-48be-aa0b-4f2f5dfbea32",
   "metadata": {},
   "source": [
    "## Testes estatisticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d5a1c-2da9-49e6-8b67-4a5607d1e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon, friedmanchisquare\n",
    "from itertools import combinations\n",
    "\n",
    "def pairwise_wilcoxon(results_dict: dict, mechanism: str, missing_rate: float, \n",
    "                      metric: str = 'NRMSE', reference_method: str = 'ISCA-k') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Realiza testes de Wilcoxon signed-rank entre o método de referência e os outros.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados do benchmark\n",
    "        mechanism: 'MCAR', 'MAR' ou 'MNAR'\n",
    "        missing_rate: Missing rate específico (e.g., 0.30)\n",
    "        metric: Métrica a comparar ('R2', 'NRMSE', 'Pearson', 'Accuracy')\n",
    "        reference_method: Método de referência (default: 'ISCA-k')\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com comparações e p-values\n",
    "    \"\"\"\n",
    "    comparisons = []\n",
    "    \n",
    "    # Obter scores do método de referência\n",
    "    key_ref = (mechanism, missing_rate, reference_method)\n",
    "    if key_ref not in results_dict:\n",
    "        print(f\"Método de referência {reference_method} não encontrado\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    ref_scores = [run[metric] for run in results_dict[key_ref] if not np.isnan(run[metric])]\n",
    "    \n",
    "    if len(ref_scores) < 2:\n",
    "        print(f\"Dados insuficientes para {reference_method}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Comparar com outros métodos\n",
    "    for method in METHOD_ORDER:\n",
    "        if method == reference_method:\n",
    "            continue\n",
    "        \n",
    "        key = (mechanism, missing_rate, method)\n",
    "        if key not in results_dict:\n",
    "            continue\n",
    "        \n",
    "        method_scores = [run[metric] for run in results_dict[key] if not np.isnan(run[metric])]\n",
    "        \n",
    "        if len(method_scores) < 2 or len(ref_scores) != len(method_scores):\n",
    "            continue\n",
    "        \n",
    "        # Wilcoxon signed-rank test\n",
    "        try:\n",
    "            statistic, p_value = wilcoxon(ref_scores, method_scores)\n",
    "            \n",
    "            # Determinar significância\n",
    "            if p_value < 0.001:\n",
    "                significance = \"***\"\n",
    "            elif p_value < 0.01:\n",
    "                significance = \"**\"\n",
    "            elif p_value < 0.05:\n",
    "                significance = \"*\"\n",
    "            else:\n",
    "                significance = \"n.s.\"\n",
    "            \n",
    "            # Calcular diferença média\n",
    "            diff = np.mean(ref_scores) - np.mean(method_scores)\n",
    "            \n",
    "            # Para NRMSE, menor é melhor (inverter interpretação)\n",
    "            if metric == 'NRMSE':\n",
    "                better = reference_method if diff < 0 else method\n",
    "            else:\n",
    "                better = reference_method if diff > 0 else method\n",
    "            \n",
    "            comparisons.append({\n",
    "                'Comparison': f\"{reference_method} vs {method}\",\n",
    "                'Reference_Mean': f\"{np.mean(ref_scores):.4f}\",\n",
    "                'Other_Mean': f\"{np.mean(method_scores):.4f}\",\n",
    "                'Difference': f\"{diff:.4f}\",\n",
    "                'p-value': f\"{p_value:.4f}\",\n",
    "                'Significance': significance,\n",
    "                'Better': better\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erro no teste {reference_method} vs {method}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(comparisons)\n",
    "\n",
    "\n",
    "def display_statistical_tests(results_dict: dict, mechanism: str, missing_rate: float,\n",
    "                              dataset_name: str, metric: str = 'NRMSE'):\n",
    "    \"\"\"\n",
    "    Exibe tabela de testes estatísticos com Plotly.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados\n",
    "        mechanism: 'MCAR', 'MAR' ou 'MNAR'\n",
    "        missing_rate: Missing rate específico\n",
    "        dataset_name: Nome do dataset\n",
    "        metric: Métrica a analisar\n",
    "    \"\"\"\n",
    "    df = pairwise_wilcoxon(results_dict, mechanism, missing_rate, metric, reference_method='ISCA-k')\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(f\"Sem dados para testes estatísticos - {mechanism} {int(missing_rate*100)}%\")\n",
    "        return\n",
    "    \n",
    "    # Criar tabela Plotly\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(\n",
    "            values=list(df.columns),\n",
    "            fill_color='#2E86AB',\n",
    "            font=dict(color='white', size=12, family='Arial'),\n",
    "            align='center',\n",
    "            height=35,\n",
    "            line_color='white'\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[df[col] for col in df.columns],\n",
    "            fill_color=[['#f8f9fa', 'white'] * len(df)],\n",
    "            font=dict(size=11, family='Arial'),\n",
    "            align='center',\n",
    "            height=28,\n",
    "            line_color='#e0e0e0'\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"{dataset_name} - {mechanism} {int(missing_rate*100)}% - Statistical Tests ({metric})\",\n",
    "        title_font_size=14,\n",
    "        height=min(400, 150 + len(df) * 30),\n",
    "        margin=dict(l=20, r=20, t=60, b=20)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Interpretação textual\n",
    "    print(f\"\\nInterpretação dos resultados ({metric}):\")\n",
    "    print(\"Significância: *** p<0.001, ** p<0.01, * p<0.05, n.s. = não significativo\")\n",
    "    \n",
    "    significant_wins = df[df['Significance'] != 'n.s.']\n",
    "    if len(significant_wins) > 0:\n",
    "        print(f\"\\nISCA-k apresenta diferenças significativas em {len(significant_wins)}/{len(df)} comparações:\")\n",
    "        for _, row in significant_wins.iterrows():\n",
    "            print(f\"  - vs {row['Comparison'].split('vs')[1].strip()}: p={row['p-value']} {row['Significance']}\")\n",
    "    else:\n",
    "        print(\"\\nNenhuma diferença estatisticamente significativa detectada.\")\n",
    "\n",
    "\n",
    "def friedman_test_all_methods(results_dict: dict, mechanism: str, missing_rate: float,\n",
    "                               metric: str = 'NRMSE') -> dict:\n",
    "    \"\"\"\n",
    "    Realiza teste de Friedman para comparar múltiplos métodos.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados\n",
    "        mechanism: 'MCAR', 'MAR' ou 'MNAR'\n",
    "        missing_rate: Missing rate específico\n",
    "        metric: Métrica a analisar\n",
    "    \n",
    "    Returns:\n",
    "        Dict com estatística e p-value\n",
    "    \"\"\"\n",
    "    # Coletar scores de todos os métodos\n",
    "    all_scores = []\n",
    "    methods_with_data = []\n",
    "    \n",
    "    for method in METHOD_ORDER:\n",
    "        key = (mechanism, missing_rate, method)\n",
    "        if key not in results_dict:\n",
    "            continue\n",
    "        \n",
    "        scores = [run[metric] for run in results_dict[key] if not np.isnan(run[metric])]\n",
    "        \n",
    "        if len(scores) >= 2:\n",
    "            all_scores.append(scores)\n",
    "            methods_with_data.append(method)\n",
    "    \n",
    "    if len(all_scores) < 3:\n",
    "        return {'error': 'Dados insuficientes (necessário ≥3 métodos)'}\n",
    "    \n",
    "    # Verificar se todos têm o mesmo número de runs\n",
    "    if len(set(len(s) for s in all_scores)) > 1:\n",
    "        return {'error': 'Número diferente de runs entre métodos'}\n",
    "    \n",
    "    try:\n",
    "        statistic, p_value = friedmanchisquare(*all_scores)\n",
    "        \n",
    "        # Calcular rankings médios\n",
    "        # Transformar para array (n_runs x n_methods)\n",
    "        data_array = np.array(all_scores).T\n",
    "        \n",
    "        # Calcular ranks: menor valor = rank 1 para NRMSE, maior valor = rank 1 para R²\n",
    "        if metric == 'NRMSE':\n",
    "            # Menor é melhor: rank 1 para o menor valor\n",
    "            ranks = np.apply_along_axis(lambda x: np.argsort(np.argsort(x)) + 1, 1, data_array)\n",
    "        else:\n",
    "            # Maior é melhor (R², Pearson, Accuracy): rank 1 para o maior valor\n",
    "            ranks = np.apply_along_axis(lambda x: np.argsort(np.argsort(-x)) + 1, 1, data_array)\n",
    "        \n",
    "        mean_ranks = ranks.mean(axis=0)\n",
    "        \n",
    "        return {\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'methods': methods_with_data,\n",
    "            'mean_ranks': mean_ranks,\n",
    "            'significant': p_value < 0.05\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "\n",
    "def display_friedman_results(results_dict: dict, mechanism: str, missing_rate: float,\n",
    "                             dataset_name: str, metric: str = 'NRMSE'):\n",
    "    \"\"\"\n",
    "    Exibe resultados do teste de Friedman.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados\n",
    "        mechanism: 'MCAR', 'MAR' ou 'MNAR'\n",
    "        missing_rate: Missing rate específico\n",
    "        dataset_name: Nome do dataset\n",
    "        metric: Métrica a analisar\n",
    "    \"\"\"\n",
    "    result = friedman_test_all_methods(results_dict, mechanism, missing_rate, metric)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"Teste de Friedman: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TESTE DE FRIEDMAN - {dataset_name}\")\n",
    "    print(f\"{mechanism} {int(missing_rate*100)}% - {metric}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    print(f\"\\nEstatística: {result['statistic']:.4f}\")\n",
    "    print(f\"p-value: {result['p_value']:.4f}\")\n",
    "    \n",
    "    if result['significant']:\n",
    "        print(\"Resultado: DIFERENÇAS SIGNIFICATIVAS entre métodos (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"Resultado: Sem diferenças significativas entre métodos\")\n",
    "    \n",
    "    print(\"\\nRanking médio (1=melhor):\")\n",
    "    for method, rank in zip(result['methods'], result['mean_ranks']):\n",
    "        print(f\"  {method}: {rank:.2f}\")\n",
    "    \n",
    "    # Ordenar por ranking\n",
    "    sorted_methods = sorted(zip(result['methods'], result['mean_ranks']), key=lambda x: x[1])\n",
    "    print(f\"\\nOrdem: {' > '.join([m for m, r in sorted_methods])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c6d55-8a3b-4384-a2ac-3d139a0f0cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
